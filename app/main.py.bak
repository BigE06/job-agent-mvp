import sqlite3
import requests
import random
from fastapi import FastAPI, HTTPException, Body
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from pathlib import Path
from bs4 import BeautifulSoup 

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

DB_PATH = "jobs.db"

def init_db():
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    # Jobs Table - Added 'status' column
    # Note: If DB exists, this won't add the column automatically. 
    # User will need to delete jobs.db to reset schema.
    c.execute('''CREATE TABLE IF NOT EXISTS saved_jobs 
                 (id INTEGER PRIMARY KEY AUTOINCREMENT, 
                  title TEXT, company TEXT, location TEXT, 
                  url TEXT, is_direct BOOLEAN, 
                  notes TEXT, score INTEGER,
                  status TEXT DEFAULT 'Saved')''') # Default status
    
    c.execute('''CREATE TABLE IF NOT EXISTS profile 
                 (id INTEGER PRIMARY KEY AUTOINCREMENT, 
                  resume_text TEXT, 
                  skills TEXT)''')
    conn.commit()
    conn.close()

init_db()

# --- STATUS ENDPOINT ---

@app.post("/api/update-status")
async def update_status(data: dict = Body(...)):
    job_id = data.get('id')
    new_status = data.get('status')
    try:
        conn = sqlite3.connect(DB_PATH)
        c = conn.cursor()
        c.execute("UPDATE saved_jobs SET status = ? WHERE id = ?", (new_status, job_id))
        conn.commit()
        conn.close()
        return {"message": "Status Updated"}
    except Exception as e:
        return {"error": str(e)}, 500

# --- PRESERVED ENDPOINTS ---

@app.post("/api/save-profile")
async def save_profile(data: dict = Body(...)):
    try:
        conn = sqlite3.connect(DB_PATH)
        c = conn.cursor()
        c.execute("DELETE FROM profile")
        c.execute("INSERT INTO profile (resume_text, skills) VALUES (?, ?)", 
                  (data.get('resume_text'), data.get('skills')))
        conn.commit()
        conn.close()
        return {"message": "Profile Saved"}
    except Exception as e:
        return {"error": str(e)}, 500

@app.get("/api/get-profile")
async def get_profile():
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    c = conn.cursor()
    c.execute("SELECT * FROM profile LIMIT 1")
    row = c.fetchone()
    conn.close()
    if row: return {"resume_text": row["resume_text"], "skills": row["skills"]}
    return {"resume_text": "", "skills": ""}

@app.post("/api/save-job")
async def save_job(job: dict = Body(...)):
    try:
        conn = sqlite3.connect(DB_PATH)
        c = conn.cursor()
        # check if exists
        c.execute("SELECT id FROM saved_jobs WHERE url = ?", (job.get('absolute_url'),))
        if c.fetchone(): return {"message": "Job already saved"}
        
        c.execute("INSERT INTO saved_jobs (title, company, location, url, is_direct, status) VALUES (?, ?, ?, ?, ?, 'Saved')",
                  (job.get('title'), job.get('company'), job.get('location'), job.get('absolute_url'), job.get('is_direct')))
        conn.commit()
        conn.close()
        return {"message": "Job Saved"}
    except Exception as e:
        return {"error": str(e)}, 500

@app.get("/api/saved-jobs")
async def get_saved_jobs():
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    c = conn.cursor()
    c.execute("SELECT * FROM saved_jobs ORDER BY id DESC")
    rows = c.fetchall()
    conn.close()
    return {"jobs": [dict(row) for row in rows]}

@app.post("/api/analyze-text")
async def analyze_text(data: dict = Body(...)):
    job_text = data.get('job_description', '')
    if not job_text: return {"error": "No text provided"}, 400
    
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    c = conn.cursor()
    c.execute("SELECT * FROM profile LIMIT 1")
    profile = c.fetchone()
    conn.close()
    
    if profile and profile['skills']:
        user_skills = [s.strip() for s in profile['skills'].split(',') if s.strip()]
        source = "Your Profile Skills"
    else:
        user_skills = ['python', 'javascript', 'communication', 'project management', 'analysis']
        source = "Default Keywords"

    found_skills = []
    missing_skills = []
    
    for skill in user_skills:
        if skill.lower() in job_text.lower(): found_skills.append(skill)
        else: missing_skills.append(skill)
            
    if len(user_skills) > 0: match_percentage = int((len(found_skills) / len(user_skills)) * 100)
    else: match_percentage = 0
        
    if match_percentage > 80: verdict = "Perfect Fit 🚀"
    elif match_percentage > 50: verdict = "Strong Contender ⭐"
    else: verdict = "Reach / Mismatch ⚠️"

    analysis_text = (f"{verdict} ({match_percentage}% Match)\n\n"
                     f"✅ Matched Skills: {', '.join(found_skills) if found_skills else 'None'}\n"
                     f"❌ Missing: {', '.join(missing_skills) if missing_skills else 'None'}\n\n"
                     f"(Based on {source})")
    
    return {"analysis": analysis_text}

@app.post("/api/generate-pack")
async def generate_pack(data: dict = Body(...)):
    return {"message": "Pack Generated", "pack_content": f"Cover Letter for {data.get('company')}..."}

@app.get("/scrape-direct/{company}")
async def scrape_direct(company: str):
    # Sanitize markdown artifact
    url = f"https://boards.greenhouse.io/{company}"
    print(f"--- SCRAPING: {url} ---")
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    try:
        response = requests.get(url, headers=headers)
        if response.status_code != 200: return {"error": "Board not found"}, 404
        soup = BeautifulSoup(response.text, 'html.parser')
        jobs = []
        seen = set()
        for a in soup.find_all('a', href=True):
            href = a['href']
            # Using relaxed /jobs/ check logic
            if '/jobs/' in href:
                if href in seen: continue
                seen.add(href)
                # Sanitize markdown artifact
                full_url = f"https://boards.greenhouse.io{href}" if href.startswith('/') else href
                title = a.text.strip()
                if not title: continue
                location = "Remote"
                try:
                    if a.parent and "|" in a.parent.text: location = a.parent.text.split("|")[-1].strip()
                except: pass
                jobs.append({"title": title, "company": company, "location": location, "absolute_url": full_url, "is_direct": True})
        return {"jobs": jobs}
    except Exception as e: return {"error": str(e)}, 500

@app.get("/jobs")
async def scrape_regular(query: str = "", location: str = ""):
    jobs = []
    for i in range(3):
        jobs.append({
            "title": f"Senior {query.capitalize()}", 
            "company": "Mock Corp", 
            "location": location, 
            # Sanitize markdown artifact
            "absolute_url": f"https://www.google.com/search?q={query}+jobs",
            "is_direct": False
        })
    return {"jobs": jobs}

base_dir = Path(__file__).resolve().parent.parent
static_dir = base_dir / "jobs_site"
if static_dir.exists(): app.mount("/app", StaticFiles(directory=str(static_dir), html=True), name="jobs_site")
